---
title: "Estadística Descriptiva"
output:
  html_document:
    toc: true
    toc_float: true
---

Los métodos estadísticos son muy importantes, ya que ellos nos permiten conocer los fenómenos que nos rodean, en aspectos que quizá no nos imaginamos. Por doquier estamos expuestos a los __datos__, y eso se extiende a científicos e ingenieros en sus actividades profesionales.  
Es muy importante definir los objetos que se constituyen en una __población__ de interés. Dicha población puede ser el conjunto de todos los productos elaborados en una fábrica un día determinado, o todos los estudiantes egresados de cierta carrera en la generación 2017, por ejemplo. Cuando tenemos acceso a todos los datos deseados de todos los miembros de una población, tenemos un __censo__.  
Sin embargo, es muy complicado poder realizar censos, ya sea por escasez de tiempo o de dinero. Por ello, se prefiere seleccionar un grupo pequeño de elementos de la población de manera aleatoria, para tener lo que se conoce como una __muestra__.  
Cabe mencionar que en ocasiones, no nos interesan todas las características de los objetos de una población. Por ejemplo, puede que hayamos reunido a 50 personas para analizar su estado de salud. Probablemente debamos preguntarles su edad, su peso, su estatura y tomarles medidas de su temperatura o presión arterial; sin embargo, no tiene sentido preguntarles qué equipo de futbol es su favorito o a qué hora pasan su noticiero predilecto. Lo que necesitamos son sus características.  
A las características podemos dividirlas en dos: __categóricas__ y __numéricas__. Las primeras son etiquetas, como los nombres o la marca de un coche. Las segundas pueden expresarse con números. Definamos una __variable__ como cualquier característica cuyo valor puede cambiar de un sujeto a otro.

## Ramas de la Estadística
Dividiremos la Estadística en dos ramas por el momento. En primera instancia, es necesario recopilar, organizar, resumir y presentar los datos; de esto se encarga la __Estadística Descriptiva__. Por otro lado, se desea extender las propiedades de la muestra hacia la población, mediante inferencias que permitan comprender mejor su comportamiento e incluso pronosticarlo; esa es la función de la __Estadística Inferencial__.

## Estadística Descriptiva en acción
La tragedia que sufrió el transbordador espacial Challenger y sus astronautas en 1986 condujo a varios estudios para investigar las razones de la falla de la misión. La atención se enfocó de inmediato en el comportamiento de los sellos anulares del motor del cohete. He aquí datos derivados de observaciones en x = temperatura del sello anular (°F) en cada encendido de prueba o lanzamiento del motor del cohete del transbordador (Presidential Commission on the Space Shuttle Challenger Accident, Vol. 1, 1986: 129-131). Los datos se presentan en una sola columna para dejar en claro que se refieren a datos univariables. Cabe mencionar que los datos se encuentran en la carpeta _datasets_ del repositorio de la asignatura en [GitHub](https://github.com/LuisEMendoza/PyE-UVM/tree/master/datasets).

```{r echo=FALSE}
temp <- scan("datasets/001-challenger.txt")
temp
```


A simple vista, los datos así mostrados no dicen gran cosa. Para ello es deseable organizarlos de manera que muestren algún comportamiento en particular. Una de las maneras más burdas es el __diagrama de tallo y hojas__, el cual se muestra a continuación:

```{r echo=FALSE}
stem(temp)
```

La explicación del diagrama es sencilla: las cifras que representan las decenas se colocan como tallos, mientras que las cifras que son unidades se vuelven hojas. Este diagrama nos permite saber de manera rápida cuáles son las temperaturas más comunes o frecuentes en nuestro conjunto de datos. También puede decirse que es un __histograma__ muy rudimentario.

A continuación se muestra un histograma:

```{r echo=FALSE}
hist(temp)
```

Y para terminar, una gráfica que posteriormente se verá cómo interpretarse y construirse es la __gráfica de caja__, la cual se muestra a continuación:

```{r echo=FALSE}
boxplot(temp)
```

Lo anterior, así como la tabla donde se muestran los datos, fue realizado con un _software_ estadístico: __R__.

## Representaciones gráficas y tabulares
Es importante mencionar que existe una notación que debemos considerar: el número de observaciones presentes en un conjunto de datos dado, se denotará por la letra _n_. También, cada observación se denotará por la letra _x_ acompañada de un subíndice que no tiene ninguna función más que identificar los datos. Es decir, representar una observación como $x_1$ no implica que esta sea la más pequeña. Con esto en mente, pasamos a revisar las representaciones gráficas de los datos.

### Diagrama de tallo y hojas
Anteriormente se habló un poco de este diagrama. Es bastante sencillo y puede aplicarse a números decimales. Veamos los pasos para producirlos:

1. Se selecciona uno o más de los primeros dígitos para el tallo. Los dígitos restantes serán las hojas.
2. Los valores del tallo se escriben de manera vertical.
3. Se anota una hoja por cada observación a la derecha del tallo.
4. Se indican las unidades para tallos y hojas en algún lugar de la gráfica.

Una gráfica de tallos y hojas da información sobre los siguientes aspectos de los datos:
- Identificación de un valor típico o representativo.
- Grado de dispersión en torno al valor típico.
- Presencia de brechas en los datos.
- Grado de simetría en la distribución de los valores.
- Número y localización de crestas.
- Presencia de valores afuera de la gráfica.

A modo de práctica, tomemos los siguientes valores que representan el flujo de agua en regaderas de Australia en litros por minuto, para _n = 130_. Se muestra también el código en _R_ para mostrar y calcular el diagrama:

```{r}
flujo <- scan("datasets/002-flujo.txt") 
flujo
```

> Si en un momento dado no sabemos exactamente dónde se encuentra el archivo fuente de nuestros datos, podemos utilizar el código `flujo <- scan(file.choose())`. Este nos abrirá una ventana que nos permitirá elegir el archivo de manera normal.

Construyamos nuestro diagrama:

```{r}
stem(flujo)
```


- ¿Cuál es una velocidad de flujo o gasto típico o representativo?
- ¿Parece estar la gráfica altamente concentrada o dispersa?
- ¿Es la distribución de valores razonablemente simétrica? Si no, ¿cómo describiría el alejamiento de la simetría?
- ¿Describiría cualquier observación como alejada del resto de los datos (un valor extremo)?

### Diagrama de puntos
Es un resumen atractivo de datos numéricos cuando el conjunto es razonablemente pequeño o existen pocos valores de datos distintos. Cada observación está representada por un punto sobre la ubicación correspondiente en una escala de medición horizontal. Cuando un valor ocurre más de una vez, existe un punto por cada ocurrencia y estos puntos se apilan verticalmente. Como con la gráfica de tallos y hojas, una gráfica de puntos da información sobre la localización, dispersión, extremos y brechas. Usemos como ejemplo el conjunto de las temperaturas del _Challenger_.

```{r}
stripchart(temp, method = 'stack')
```

### Tablas de frecuencia
Vamos a trabajar ahora con datos categóricos. Cuando hablamos de categorías, hablamos de variables nominales, es decir, nombres o etiquetas. Por lo tanto, el tratamiento que debemos darles difiere de los números. Por principio de cuentas, debemos tabularlos. Esto se consigue con una tabla de frecuencias. Primero leemos el archivo con los datos:

```{r}
marca <- c(read.table("datasets/003-marca.txt"))
marca
```
Construir una tabla de frecuencias implica contar el número de veces que una etiqueta se repite. A esa cantidad se le conoce como la frecuencia absoluta. Enseguida creamos la tabla:

```{r}
mar <- table(marca)
mar
```

De aquí en delante podemos complementar la tabla de frecuencias agregando la frecuencia acumulada y la relativa. Llenémosla:

Marca | Frecuencia | Frec. acumulada | Frec. relativa | Frec. rel. acumulada
:---|:---:|:---:|:---:|:---:
Apple | 8 | 8 | 0.25 | 0.25
HTC | 1 | 9 | 0.03125 | 0.28125
Huawei | 4 | 13 | 0.125 | 0.40625
LG | 1 | 14 | 0.03125 | 0.4375
Motorola | 5 | 19 | 0.15625 | 0.59375
Nokia | 2 | 21 | 0.0625 | 0.65625
Samsung | 10 | 31 | 0.3125 | 0.96875
Xiaomi | 1 | 32 | 0.03125 | 1

La frecuencia acumulada, también conocida como frecuencia absoluta acumulada, se corresponde con la suma de las frecuencias que han aparecido hasta el momento. Por ejemplo, la frecuencia acumulada para Huawei es de 13, que se corresponde con los 8 de Apple, la única de HTC y las 4 unidades de la propia Huawei. La frecuencia relativa es el cociente de la frecuencia absoluta entre el total de observaciones, por ejemplo, para Apple se corresponde con el resultado de dividir sus 8 unidades entre el total, que fueron 32. Esos cocientes se acumulan para obtener la frecuencia relativa acumulada.

Ahora bien, la única ocasión en la que se recomienda utilizar un diagrama de sectores -también conocido como de pastel-, es cuando se trabaja con variables nominales (etiquetas o categorías). Para su construcción, basta con tomar la frecuencia relativa de cada categoría y multiplicarla por 360, que son la totalidad de los grados contenidos en un círculo completo.

Marca | Grados
:--- | :---:
Apple | 90
HTC | 11.25
Huawei | 45
LG | 11.25
Motorola | 56.25
Nokia | 22.5
Samsung | 112.5
Xiaomi | 11.25

El círculo se divide en las 8 secciones con magnitudes en grados ya calculadas. El diagrama quedaría de la siguiente manera (elaborada con R):

```{r}
pie(mar)
```

### Distribuciones de frecuencia
En ocasiones es necesario trabajar con valores que no se repiten tanto, como en el caso que se presenta enseguida.

Las compañías eléctricas requieren información sobre el consumo de los clientes para obtener pronósticos precisos de demandas. Investigadores de Wisconsin Power and Light determinaron el consumo de energía (BTU) durante un periodo particular con una muestra de 90 hogares calentados con gas. Las siguientes observaciones son el consumo promedio:

```{r}
Consumo <- scan("datasets/004-horno.txt")
Consumo
```

Es notorio que los valores son bastantes y que rara vez se repiten. Cuando nuestros datos son así, se recomienda agruparlos en una distribución de frecuencias. Dicha distribución divide los datos en intervalos y cuenta las ocasiones en que los mismos datos se encuentran entre estos intervalos. A estos últimos se les conoce también como __clases__. 

El primer paso es determinar el número de datos. En esta ocasión ya lo sabemos, son 90. Si en un momento dado no fuera así, podemos contarlos con el siguiente comando:
```{r}
n <- length(Consumo)
n
```

Entonces toca definir el número de clases. No existe una regla inamovible para hacer esto, pero usualmente se utiliza uno de los tres criterios siguientes:

__La raíz cuadrada del número de datos__. Al momento de contar el total de observaciones, se calcula la raíz de dicho valor.
$$
total\ de\ clases \ \approx \sqrt {total\ de\ observaciones}
$$
```{r}
sqrt(n)
```
Redondeamos hacia arriba y definimos el número de clases como diez.

__La Regla de Sturges__. Fue propuesta en 1926 y es la que utilizan en su mayoría los programas estadísticos para el cálculo y elaboración de histogramas y distribuciones de frecuencia. Su fórmula es:

$$
k=1+log_2(n)
$$
Donde $k$ es el número de clases y $n$ es el tamaño de muestra. Para este caso, el número de clases sería:

```{r}
ksturges <- 1+log2(n)
ksturges
```

__La regla de Scott__. Propuesta en 1992, utiliza la distribución normal como base y recomienda utilizar la expresión:

$$
k=(2n)^{1/3}
$$

```{r}
kscott <- (2*n)^(1/3)
kscott
```

Con el número de  clases ya definido, lo siguiente en realizar será determinar la anchura o límites de la clase. Igual que en el número de clases, no existe una regla infalible para hacerlo, y muchas veces puede dejarse a criterio de la persona que realiza el análisis. Se pueden mencionar dos pautas:

__Al tanteo__. En este caso obtenemos el número más bajo y el más alto y buscamos unos límites que nos permitan englobar dichos números sin que quepa la posibilidad de dejar algún valor fuera. Por ejemplo, el número más bajo (o mínimo) es $2.97$, por lo cual podríamos definir $2.5$ o incluso $2$ como límite inferior de la primera clase; en el otro extremo, el valor máximo es $18.26$, por lo que podremos dejar $18.5$, $19$ o incluso $20$ como límite superior. Cabe mencionar que el $20$ sería para asegurarnos de tener clases «simétricas».

__Por fórmula__. En este caso, prescindimos de estar buscando un número adecuado y lo calculamos directamente con la siguiente expresión:

$$
Anchura=\frac {R}{NC}
$$
Donde $R$ es el rango (esto es, la diferencia entre el valor máximo y el mínimo) y $NC$ es el número de clases. Supongamos que queremos 9 clases, entonces encontremos el máximo y el mínimo:

```{r}
max(Consumo)
min(Consumo)
```

Y ahora calculamos la anchura:
```{r}
anchura <- (max(Consumo)-min(Consumo))/9
anchura
```

Al ser un valor decimal, se recomienda redondearse hacia arriba, quedando en dos. Así que nuestras clases comenzarían en 2 como límite inferior, y aumentarán de dos en dos hasta alcanzar el límite superior: 20.

Podríamos graficar el histograma directamente, pero queremos obtener la distribución de frecuencias también. Para ello, ejecutamos la función `hist` pero con la opción de no mostrarlo y con el número de clases que hayamos definido.

```{r}
df <- hist(Consumo, nclass = 9, plot = FALSE, right = FALSE)
df
```

```{r}
intervalos <- df$breaks
clases <- cut(Consumo, breaks = intervalos, dig.lab = 2, right = FALSE)
tabla.frec <- table(clases)
tabla <- as.data.frame(tabla.frec)
tabla
```

El histograma quedaría así:

```{r}
hist(Consumo, right = FALSE)
```

### Histograma con anchos de clase desiguales
Es posible que las clases de ancho-igual no sean una opción sensible si un conjunto
de datos “se alarga” hacia un lado o el otro. Como ejemplo, veamos el siguiente caso:

En un estudio de ruptura de la urdimbre durante el tejido de telas (Technometrics, 1982: 63), se sometieron a prueba 100 muestras de hilo. Se determinó el número de ciclos de esfuerzo hasta ruptura para cada muestra de hilo y se obtuvieron los datos siguientes:

```{r}
hilos <- scan("datasets/005-hilos.txt")
hilos
```

Trazamos nuestro histograma:
```{r}
hist(hilos, breaks = 'Sturges', right = FALSE)
```

Aquí podemos ver que el histograma se alarga demasiado hacia la derecha, mostrando clases con muy pocas observaciones. En este tipo de ocasiones, puede utilizar un histograma basado en densidades, más que en frecuencias. Ello implica obtener anchuras diferentes.

```{r}
hist(hilos, breaks = c(0,50, 100,150, 200,400,600,900), right = FALSE)
```

Después de determinar las frecuencias y las frecuencias relativas, se calcula la altura
de cada rectángulo con la fórmula
$$
altura=\frac{frecuencia\ relativa\ de\ clase}{ancho\ de\ clase}
$$

Las alturas del rectángulo resultante en general se conocen como densidades y la escala vertical es la escala de densidades. Esta prescripción también funcionará cuando los anchos de clase son iguales.

```{r}
hist(hilos, breaks = c(0,50, 100,150, 200,400,900), right = FALSE)
```


## Medidas de tendencia
Los resúmenes visuales son bastante atractivos y deseables para obtener percepciones o nociones preliminares. Sin embargo, para un mejor análisis es mejor obtener un resumen basado en la interpretación de medidas numéricas. Suponga que un conjunto de observaciones de la forma $x_1$, $x_2$, $x_3$, donde cada $x_i$, (la $i$ es solo un indicador o índice, es decir, no tiene valor numérico). ¿Qué es lo que nos importa saber de ellos?

### La media
La medida más conocida y útil del centro es la _media_ o promedio aritmético del conjunto. Si los números se denotan como $x_i$, entonces la media se indica como $\bar{x}$. Se calcula con la siguiente expresión:

$$
\bar{x}=\frac{x_1+x_2+ ... +x_n}{n}=\frac{\sum_{i=1}^n x_i}{n}
$$
Podemos escribir el numerador de la expresión informalmente como $\sum x_i$, entendiendo que la suma incluye todas las observaciones muestrales.

#### Ejemplo
Veamos el caso de un proceso de elaboración de calderas de acero. El metal se encuentra sometido a esfuerzo cáustico, el cual causa agrietamientos, cuya longitud se recogió en la siguiente lista:

```{r}
grietas <- scan("datasets/007-caustic.txt")
grietas
```

Sumemos todos los valores de la lista, y enseguida dividamos entre el número de observaciones.

Lo primero lo logramos con:

```{r}
suma <- sum(grietas)
suma
```

El tamaño muestral se obtiene con:

```{r}
observaciones <- length(grietas)
observaciones
```

Ahora sí, calculemos la media:

```{r}
media <- suma/observaciones
media
```

R tiene la función ya predeterminada:

```{r}
mean(grietas)
```

#### Ejercicio

Trabajemos ahora con otro conjunto de valores:

```{r}
m <- scan("datasets/004-horno.txt")
m
stem(m)
```

```{r}
mean(m)
```


Por lo anterior, podemos entender la media como una especie de balanza o punto de apoyo que nos permite equilibrar los pesos (las observaciones) en un eje horizontal. Ahora bien, este concepto tiene un gran problema: es muy sensible a valores extremos. Por ejemplo, consideremos el conjunto de temperaturas del _Challenger_:

```{r}
temp <- scan("datasets/001-challenger.txt")
temp
```

Como ya habíamos visto, la media resultaba ser:

```{r}
mean(temp)
```

Pero si agregamos un dato muy _extraño_ o no esperado, aún más que el propio 31, como por ejemplo un catastrófico 350:

```{r}
n.media <- (sum(temp)+350)/(length(temp)+1)
n.media
```

Nuestra media se recorre demasiado hacia la derecha del conjunto, muy cerca del final de los datos, con el afán de «equilibrarlos». Cuando se trabaja con observaciones que pueden tener muchos datos extremos, es recomendable no elegir la media para representar su tendencia. En su lugar, se recomienda la __mediana__.


### La mediana
La mediana es una medida de tendencia que literalmente, parte al conjunto de observaciones (previamente ordenadas de menor a mayor) por la mitad. Es decir, es el valor de en medio. Se denota por $\tilde x$. La mediana se obtiene de dos maneras, dependiendo del número de observaciones $n$.

> Si el número de observaciones es impar, la mediana es el valor que se encuentra en la siguiente posición:

$$
\tilde x=valor\ en\ el\ lugar\  \#  \frac{n+1}{2} \ ordenado
$$

>Si es par, debemos localizar los dos valores centrales y calcular su media; el resultado es la mediana.

$$
\tilde x=promedio\ de\ los \ valores\   \#  \frac{n}{2}\ y\ el\ \# \frac{n}{2}+1 \ ordenados
$$

#### Ejemplo 1
El riesgo de desarrollar deficiencia de hierro es especialmente alto durante el embarazo. El problema con la detección de tal deficiencia es que algunos métodos para determinar el estado del hierro pueden ser afectados por el estado de gravidez mismo. Considérense las siguientes observaciones de concentración de receptores de transferrina de una muestra de mujeres con evidencia de laboratorio de anemia por deficiencia de hierro evidente (“Serum Transferrin Receptor for the Detection of Iron Deficiency in Pregnancy”, Amer. J. of Clinical Nutrition, 1991: 1077-1081):

```{r}
Fe <- scan("datasets/008-hierro.txt")
Fe
```

Lo primero que debemos hacer es ordenar los datos de menor a mayor. Lo conseguimos en _R_ con lo siguiente:

```{r}
orden <- sort(Fe)
orden
```

Ahora contamos el total de observaciones:
```{r}
n <- length(orden)
n
```
Tenemos un conjunto par de observaciones, así que obtengamos los números que ocupan las posiciones centrales. Primero, el primero:

```{r}
n1 <-orden[n/2]
n1
```

Y ahora el segundo:
```{r}
n2 <- orden[(n/2)+1]
n2
```

Calculemos su media:

```{r}
mediana1 <- (n1+n2)/2
mediana1
```

#### Ejemplo 2

La exposición a productos microbianos, especialmente endotoxina, puede tener un impacto en la vulnerabilidad a enfermedades alérgicas. El artículo (“_Dust Sampling Methods for Endotoxin-An Essential, But Underestimated Issue_”, Indoor Air, 2006: 20-27) consideró temas asociados con la determinación de concentración de endotoxina. Se obtuvieron dos tipos de muestra para datos sobre concentración (EU/mg) en polvo asentado: hogares urbanos (U) y otra de casas campestres (C), las cuales fueron amablemente suministrados por los autores del artículo citado.

A continuación se muestra la muestra urbana:

```{r}
u <- scan("datasets/009-toxinas-U.txt")
u
```
Obtenemos el tamaño muestral:
```{r}
tamano <- length(u)
tamano
```

En este caso tenemos un tamaño muestral impar, así que resulta más sencillo obtener la mediana. Ordenamos la muestra y enseguida obtenemos la posición del número central. La lista ordenada:

```{r}
u.ordenada <- sort(u)
u.ordenada
```

La posición del valor central:
```{r}
mediana2 <- u.ordenada[(tamano+1)/2]
mediana2
```

Este es el valor de la mediana. Hasta ahora hemos estado utilizando comandos aritméticos para determinarla, sin embargo, existe un comando único en _R_ para obtenerla. Para el primer conjunto de valores:

```{r}
median(Fe)
```

Para el segundo:

```{r}
median(u)
```
La media $\mu$ y la mediana $\tilde \mu$ poblacionales en general no serán idénticas. Si la distribución de la población es positiva o negativamente asimétrica, entonces $\mu \neq \tilde \mu$. Cuando éste es el caso, al hacer inferencias primero se debe decidir cuál de las dos características de la población es de mayor interés y luego proceder como corresponda.

![Solo en distribuciones normales simétricas](images/018-median.jpg)

La mediana también puede calcularse con la función `quantile`, ya que a final de cuentas, es el segundo cuartil:

```{r}
quantile(u, probs = 0.5)
```

Podemos calcular cualquier cuantil (también conocido como percentil), siendo los más comunes los cuartiles:

```{r}
quantile(u, probs = 0.25)
quantile(u, probs = 0.75)
quantile(u, probs = 0.20)
```

Los cuartiles dividen el conjunto de datos precisamente en cuartas partes. Como puede deducirse de lo anterior, el primer cuartil toma solo el primer 25% de las observaciones, dejando libre el restante 75%. El tercer cuartil hace lo mismo pero a la inversa: toma el 25% superior y deja libres los primeros datos, correspondiendo al 75% inferior.

## Medidas de dispersión
En ocasiones queremos saber qué tan dispersos se encuentran los valores de una muestra entre sí o con respecto a la media. Para ello utilizamos las medidas de dispersión. Usualmente son tres: el __rango__, la __varianza__ y la __desviación estándar__. 

### El rango
Considera solo la diferencia entre el valor máximo y el mínimo, por lo que no toma en cuenta los valores intermedios:

```{r}
u
rango <- max(u) - min(u)
```

```{r}
rango
```

### La varianza
Su fórmula es la siguiente:

$$
s^2=\frac{\sum_{i=1}^n (x_i-\bar x)^2}{n-1}
$$

Y su obtención se consigue con:

```{r}
var(u)
```

### La desviación estándar
No es más que la raíz cuadrada de la varianza. Se obtiene directamente con:

```{r}
sd(u)
```

#### Ejercicio

La cantidad de luz reflejada por las hojas ha sido utilizada para varios propósitos, incluidas la evaluación del color del césped, la estimación del estado del nitrógeno y la medición de la biomasa. El artículo (“_Leaf Reflectance-Nitrogen-Chlorophyll Relations in Buffel-Grass_”, Photogrammetric Engr. and Remote Sensing, 1985: 463-466) dio las siguientes observaciones obtenidas por medio de espectrofotogrametría, de la reflexión de las hojas en condiciones experimentales.

```{r}
hojas <- read.table("datasets/011-hojas.txt", header = T)
hojas
```

> Nótese que para capturar un archivo que no consta de una sola columna, en R podemos utilizar la función `read.table` con la función `header = TRUE`. Para _llamar_ una sola columna podemos utilizar la nomenclatura `nombredelatabla$nombredelacolumna`, o bien `nombredelacolumna[,númerodecolumna]`.

La idea es calcular la varianza de las observaciones. Para ello debemos invocar solamente la columna que nos interesa, que es la que contiene los datos como tales: 

```{r}
datos <- hojas$xi
datos
```

```{r}
datos2 <- hojas[,2]
datos2
```

Calculemos la varianza de dicha columna:

```{r}
var(datos)
```

> Podemos calcular directamente sobre la columna:

```{r}
var(hojas[,2])
```

Calculemos ahora la desviación estándar:

```{r}
sd(hojas[,2])
```

## Medidas de forma

### Gráficas de caja
Un gráfico que ha demostrado ser muy útil en la representación efectiva de los datos es precisamente el diagrama de caja. Este se construye a partir de medidas que son _resistentes_ a los datos extremos, es decir, la mediana y los cuartiles. Las características que nos muestra el gráfico son el centro, la dispersión, la asimetría y la presencia -o ausencia- de observaciones extremas. Para ello es necesario encontrar los siguientes números: 
- Observaciones más pequeñas
- Primer cuartil
- Mediana
- Tercer cuartil
- Observaciones más grandes

La distancia existente entre cuartiles es lo que se conoce como __rango intercuartílico__, y es representado por la altura de la caja dibujada si el es vertical (por lo tanto, se corresponde a la anchura si el eje fuera horizontal). 

#### Ejercicio

Se utilizó ultrasonido para reunir los datos de corrosión adjuntos de la placa de piso de un tanque elevado utilizado para almacenar petróleo crudo (“Statistical Analysis of UT Corrosion Data from Floor Plates of a Crude Oil Aboveground Storage Tank”, Materials Eval., 1994: 846-849); cada observación es la profundidad de picadura más grande en la placa, expresada en milésimas de pulgada.

```{r}
us <- scan("datasets/012-ultrasonido.txt")
us
```

Obtengamos las medidas mencionadas anteriormente:
```{r}
min(us) #número más pequeño
quantile(us, probs = 0.25) #primer cuartil
median(us) #mediana
quantile(us, probs = 0.75) #tercer cuartil
max(us) #número más grande
```
> Podemos obtener dichas medidas (e incluso la media) con un simple comando en R:

```{r}
summary(us)
```


Trazamos un eje (puede ser horizontal o vertical) con la escala adecuada, en este caso podemos dividir de diez en diez, partiendo desde 30 hasta llegar a 130. Sobre dicho eje trazamos un rectángulo cuyo lado izquierdo se encuentre en el primer cuartil (72.5); y su lado derecho en el tercero (96.5). Dentro del rectángulo dibujaremos la línea que represente la mediana (90). Al finalizar esto, trazamos una línea que salga del lado izquierdo del rectángulo y termine en el número más pequeño (40). Así mismo, trazamos una línea que llegue al número más alto (125). La gráfica debería quedarnos más o menos como sigue:

```{r}
boxplot(us)
```

### Gráficas de caja con datos atípicos
La presencia de datos extremos puede ser contraproducente. Por lo tanto no es deseable tomarlos en cuenta si se quiere asumir que la distribución de los datos es _normal_. Por ello, debemos indicar explícitamente la presencia de datos atípicos, de los cuales podemos distinguir entre moderados y extremos. Para esto, debemos calcular el rango intercuartílico ($f_s$) y multiplicarlo por $1.5$. Cualquier dato que se encuentre a más de $1.5f_s$ del extremo de la caja se considera atípico o apartado. De entre estos valores apartados, asumiremos que todos son moderados con excepción de los que se encuentren a más de $3f_s$ del cuartil más cercano.

#### Ejercicio con datos apartados
Los efectos de descargas parciales en la degradación de materiales para cavidades aislantes tienen implicaciones importantes en relación con las duraciones de componentes de alto voltaje. Considérese la siguiente muestra de 25 anchos de pulso de descargas lentas en una cavidad cilíndrica de polietileno. (Estos datos son consistentes con un histograma de 250 observaciones en el artículo “Assessment of Dielectric Degradation by Ultrawide-band PD Detection”, IEEE Trans. on Dielectrics and Elec. Insul., 1995: 744-760.) El autor del artículo señala el impacto de una amplia variedad de herramientas estadísticas en la interpretación de datos de descarga.

```{r}
desc <- scan("datasets/013-descargas.txt")
desc
```

Calculemos las 5 medidas primordiales para el gráfico de caja:

```{r}
summary(desc)
```

Es muy notorio que el dato inferior está muy alejado del primer cuartil. ¿Qué tan alejado se encuentra? ¿Habrá más en la parte inferior? ¿Habrá alguno en la superior? Calculemos $f_s$ y luego multipliquémoslo por 1.5:
```{r}
fs <- quantile(desc, probs = 0.75)-quantile(desc, probs = 0.25)
fs
lim <- fs*1.5
lim
```

Toda observación que se encuentre separada por más de 9.75 de cualquier cuartil será un dato atípico. Calculemos los valores que delimitarán lo atípico en ambos extremos:

```{r}
lims <-quantile(desc, probs = 0.75)+lim
lims
```
```{r}
limi <- quantile(desc, probs = 0.25)-lim
limi
```

Cualquier observación por encima de 106.45 o por debajo de 80.45 será atípica. Notemos que tenemos uno solo por encima,el máximo valor de hecho: $113.5$. Por la parte inferior existen cuatro: 5.3, 8.2, 13.8 y 74.1.

Ahora veamos si tenemos datos atípicos extremos en la parte superior:
```{r}
limsx <- quantile(desc, probs = 0.75)+3*fs
limsx
```
Ni siquiera el dato máximo supera esta marca. Así que no hay datos extremos en la parte superior de la caja.

En seguida veamos en la parte inferior:
```{r}
limix <- quantile(desc, probs = 0.25)-3*fs
limix
```

Podemos darnos cuenta que en este lado sí existen datos extremos: 5.3, 8.2 y 13.8. Por lo tanto, el rectángulo (la caja) sí se traza de manera normal, pero las líneas (los bigotes) no. Sus extremos llegan solamente hasta los datos que se consideran normales: 85.3 en el lado inferior y 106.0 en el superior. Las observaciones apartadas moderadas se trazan como círculos oscuros y las apartadas extremas como círculos claros. Cabe mencionar que algunos paquetes estadísticos no distinguen los datos anormales moderados de los extremos, como es el caso de R. Tracemos nuestra caja:
```{r}
boxplot.default(desc, horizontal = T)
```


## Análisis de dispersión

### Introducción

En el mundo de las matemáticas estamos muy acostumbrados a relacionar un par de variables de manera _determinística_, es decir, sabemos que cuando modificamos una variable, esta hará que otra cambie. Un buen ejemplo de esto es cuando llenamos el tanque de gasolina, sabemos que si compramos 20 litros, nos costará veinte veces el precio de un litro. Puesto que el litro se encuentra, supongamos a $\$20.00$, entonces pagaremos veinte veces ese precio: $\$400.00$ en total. Sin embargo, no siempre las relaciones entre variables son así de sencillas. Habrá pares de variables cuyo comportamiento sea solo en parte explicado por su contraparte. Volviendo al caso de la gasolina, con esos veinte litros que cargamos podremos recorrer una cantidad de kilómetros. ¿Podemos saber exactamente cuántos? No es fácil, ya que el rendimiento de la gasolina depende no solamente de la distancia, si no de otros factores, como por ejemplo el tipo de conducción, el terreno recorrido, las condiciones atmosféricas, etc. Es este segundo tipo de relación entre variables el que nos interesa: _una relación no determinística_. Para estudiar estas relaciones contamos con el análisis de regresión, que es parte de la Estadística.

Sabemos que existe una relación determinística entre dos variables que se puede representar mediante la expresión:

$$
\hat y=\beta_0+\beta_1x
$$

Este arreglo traza en el plano una línea recta que tendra a $\beta_0$ como su intersección en el eje $y$ o valor de arranque, mientras que el coeficiente $\beta_1$ se corresponde con la pendiente o cambio promedio de la propia $y$. Consideremos pues, que de acuerdo a la expresión de arriba, $y$ es la variable dependiente, pronosticada, explicada o influenciada, mientras $x$ es la variable independiente, pronosticadora, explicativa o influenciadora. En otras palabras, $\beta_0$ es el valor de $y$ cuando $x$ es nula; por otro lado, por cada unidad que $x$ cambie, $y$ estará cambiando $\beta_1$ veces. A partir de ahora se trabajará con datos bivariables, es decir, datos con valores correspondientes a $x$ y $y$. El primer paso para comenzar un análisis de regresión es construir un gráfico de puntos bivariable, donde cada par $(x_i,y_i)$ será representado por un punto en el plano $xy$.

#### Ejemplo 1

Los problemas visuales y musculoesqueléticos asociados con el uso de terminales con pantallas de visualización (VDT, por sus siglas en inglés) se han vuelto un tanto comunes en años recientes. Algunos investigadores se han enfocado en la dirección vertical de la mirada fija como causa del cansancio e irritación de los ojos. Se sabe que esta relación está estrechamente relacionada con el área de la superficie ocular (OSA, por sus siglas en inglés), así que se requiere un método de medir el área de la superficie ocular. Los datos representativos adjuntos sobre $y=$ OSA (cm2) y $x=$ ancho de la fisura palprebal (es decir, el ancho horizontal de la apertura del ojo (en cm) se tomó del artículo “Analysis of Ocular Surface Area for Comfortable VDT Workstation Layout” (Ergonomics, 1996: 877-884). No se da el orden en el cual se obtuvieron las observaciones, así que por conveniencia aparecen en orden creciente de los valores $x$.

```{r}
osa <- read.table("datasets/014-osa.txt", header = T)
(osa)
```

El conjunto de datos completo se encuentra en la carpeta _datasets_ en la página de la asignatura en [GitHub](https://github.com/LuisEMendoza/PyE-UVM/tree/master/datasets). Ahora bien, procedamos a trazar el gráfico:

```{r}
ancho <- osa$x
area <- osa$y
plot(ancho, area)
```

Podemos observar en este gráfico que existen valores de $x$ que se repiten, pero no les corresponden los mismos valores en $y$, como es el caso de ambos $0.75$ en $x$, cuyos valores difieren para $y$ en cuanto a que uno es de $1.80$ y el segundo es de $1.74$. Con esto podemos notar que los valores de $y$ no dependen exclusivamente de $x$, sino de otros factores que de momento, no conocemos. Sin embargo, podemos deducir o pronosticar un valor de $y$ que corresponda a uno de $x$ que propongamos; debido a que es notorio que sí existe una relación lineal entre ambas variables, ya que al aumento de una, le corresponde el de la otra. Debemos encontrar una expresión que represente esta relación y su fuerza.

Dicha expresión para un modelo determinístico $\hat y=\beta_0+\beta_1x$, indica que el valor observado de $y$ es una función lineal de $x$. La generalización apropiada de esto a un modelo probabilístico supone que el valor esperado de $Y$ es una función lineal de $x$, pero que con $x$ fija, la variable $Y$ difiere de su valor esperado en una cantidad aleatoria.

La ecuación de regresión cambia entonces para agregar el elemento probabilístico: el __término de error aleatorio__ o __desviación aleatoria__.

$$
Y=\beta_0+\beta_1x+\epsilon
$$
Donde $\epsilon$ es precisamente el error aleatorio. Sin este término, todos los puntos se encontrarían sobre la línea que representa la ecuación misma, pero al trazarla para este caso, podremos darnos cuenta de que no es así. Podemos pensar entonces que la distancia entre cada punto que representa un par de observaciones a la línea es precisamente el término de error aleatorio que le corresponde a dicho par. Tracemos una línea que represente la relación entre variables:

```{r echo=F}
r1 <- lm(area ~ ancho)
plot(ancho,area)
abline(r1)
```

Aquí se observa precisamente lo que se mencionaba nateriormente, la distancia que existe entre cada punto y la línea que representa la relación lineal entre ambas variables:

### Método de mínimos cuadrados

Se remite a los tiempos del matemático alemán __Gauss__, y consiste de las siguientes fórmulas para la obtención de la ecuación de regresión:

$$
b_1=\frac{\sum x_iy_i-\frac{\sum x_i\sum y_i}{n}}{\sum x_i^2-\frac{(\sum x_i)^2}{n}} \\
\ \\
b_0=\bar y-b_1 \bar x
$$


#### Ejemplo 2

El concreto sin finos, hecho de un agregado grueso uniformemente graduado y una pasta de cemento y agua, es benéfico en áreas propensas a lluvias intensas debido a sus excelentes propiedades de drenaje. El artículo “Pavement Thickness Design for No-Fines Concrete Parking Lots” (J. of Transportation Engr., 1995: 476-484) empleó un análisis de mínimos cuadrados al estudiar cómo $y=$ porosidad (%) está relacionada con $x=$ peso unitario (pcf) en especímenes de concreto. Considere los siguientes datos representativos, mostrados en formato tabular conveniente para calcular los valores de los estadísticos resumidos:


```{r}
concreto <- read.table("datasets/015-concreto.txt", header = T)
concreto
```

Calculemos los valores que necesitamos para encontrar cada coeficiente. Definimos primero los vectores que utilizaremos para el análisis. El primero como `peso`:

```{r}
peso <- concreto$peso #definimos el vector "peso"
peso
```

Y el segundo como `porosidad`:
```{r}
porosidad <- concreto$porosidad #definimos el vector "porosidad"
porosidad
```

Ahora obtengamos la suma de ambos vectores y asignémoslo a sus propias variables. Primero la sumatoria de `peso`:
```{r}
peso.suma <- sum(peso)
peso.suma
```

Y luego la de `porosidad`.
```{r}
porosidad.suma <- sum(porosidad)
porosidad.suma
```

Enseguida calculamos las medias de cada vector:
```{r}
peso.media <- mean(peso)
peso.media
```

```{r}
porosidad.media <- mean(porosidad)
porosidad.media
```


Toca calcular las columnas de $peso^2$, $porosidad^2$ y $peso*porosidad$ y luego sus sumatorias. Vector `peso2`:

```{r}
peso2 <- peso^2 #elevamos peso al cuadrado
peso2
```
Su sumatoria:
```{r}
peso2.suma <- sum(peso2)
peso2.suma
```

Vector `porosidad2`:
```{r}
porosidad2 <- porosidad^2 #elevamos porosidad al cuadrado
porosidad2
```

Sumatoria de `porosidad2`:
```{r}
porosidad2.suma <- sum(porosidad2)
porosidad2.suma
```

Vector `peso*porosidad`:
```{r}
pesoxporosidad <- peso*porosidad # multiplicamos ambas columnas
pesoxporosidad
```

```{r}
pesoxporosidad.suma <- sum(pesoxporosidad)
pesoxporosidad.suma
```

Procedemos a resolver para $b_1$. Para no colocar código complicado de leer, vamos a dividir la expresión en dos cálculos, el numerador:
```{r}
num.b1 <- pesoxporosidad.suma-(peso.suma * porosidad.suma)/length(peso)
num.b1
```

Y el denominador:
```{r}
den.b1 <- peso2.suma-peso.suma^2/length(peso)
den.b1
```

Como paso final del cálculo, los dividimos:

```{r}
b1 <- num.b1/den.b1
b1
```

Lo que hicimos fue la siguiente operación:

$$
b_1=\frac{32308.59-(1640.1*299.8)/15}{179849.7-1640.1/15}=\frac{-471.542}{521.196}=-0.9047
$$

El cálculo de $b_0$ es más sencillo. Tomamos la media de la porosidad y le restamos el producto de la media del peso y $b_1_.

```{r}
b0 <- porosidad.media-b1*peso.media
b0
```

Expresado con cantidades, lo que hicimos fue:
$$
b_0=19.98-(-0.9047)*(109.34)=118.9099
$$

La ecuación de regresión para este ejercicio será entonces de la siguiente manera:

$$
\hat y=118.91-0.905x
$$

Con lo anterior, se estima que el cambio de porosidad esperado asociado con un incremento de una unidad en el peso unitario es de $\approx -0.905%$ (una reducción de 0.905%). La figura siguiente, muestra que la línea de mínimos cuadrados proporciona un excelente resumen de la relación entre las dos variables.

```{r}
r2 <- lm(porosidad~peso)
plot(peso, porosidad)
abline(r2)
```


Todo lo anterior puede ser conseguido con una sola línea de código en _R_:

```{r}
rl <- lm(porosidad~peso)
rl
```

Aquí podemos observar que `Intercept` es nuestro coeficiente $b_0$ y `peso` se corresponde con $b_1$. El siguiente paso es obtener el valor pronosticado de $y$ para un determinado valor de $x$. Para ello sustituimos en nuestra ecuación de regresión un peso de _109_ y resolvemos para $y$:

```{r}
118.9099 - 0.9047*109
```

Lo anterior implica que para un peso de 109, la porosidad esperada sería de 20.2976. Podemos sustituir tantos valores de $x$ como queramos, pero no se recomienda que sean muy diferentes a los que se encuentran originalmente en la tabla de datos. 

### Coeficiente de determinación
Mientras el valor de $b_1$ nos indica si la relación entre las dos variables es positiva, no tenemos de momento una medida que nos cuantifique qué tan fuerte es dicha relación. Para ello contamos con el coeficiente de determinación, el cual podemos obtenr a partir del coeficiente de correlación, fácil de calcular en _R_:

```{r}
coef.cor <- cor(peso, porosidad)
coef.cor
```

Este coeficiente nos indica la naturaleza de la relación. Al tener signo negativo, nos confirma lo que $b_1$ ya mostró: relación negativa. Pero aún no nos indica qué tan fuerte es dicha relación. Para ello calculamos el cuadrado de dicho coeficiente:

```{r}
coef.cor^2
```

La relación es bastante alta, ya que el coeficiente de determinación oscila en un rango muy pequeño, de cero a uno. Mientras más se acerque al uno, más exitoso es nuestro modelo explicando la naturaleza de lo que estamos estudiando. Convertido en porcentaje, nos indica que $x$ explica el 97.39% de la variación de $y$; o lo que es lo mismo, el 97.39% de la variación en la porosidad es explicada por el peso.

El coeficiente de determinación también puede ser obtenido mediante la sentencia:

```{r}
summary(rl)
```

Con la cual, bajo el nombre de _R-squared_, aparece nuestro valor ya obtenido. Es importante notar que aparecen dos de estos. El primero (_Multiple..._) es el que tomamos para modelos que solo contengan una variable dependiente y otra independiente. El segundo (_Ajusted..._) es útil para modelos multivariables.
